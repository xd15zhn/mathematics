如果约定所有的向量均为列向量，那么我认为矩阵求导在实际使用中就没有必要讨论行布局或者列布局，因为导数的推导过程和结果就决定了矩阵求导公式是唯一的。
大部分情况二阶矩阵或二维向量足以说明问题，剩下的到三阶也够了，不会出现低阶成立而高阶不成立的情况，所以下面举的例子均为三阶及以下。
若无特别说明，细斜体(如$J$)表示标量，粗斜体小写(如$\boldsymbol{x}$)表示向量，粗正体大写(如$\mathbf{A}$)表示矩阵。

# 损失函数与梯度下降法
$$
    J=\frac{1}{2}(\boldsymbol{y}-\hat{\boldsymbol{y}}(\boldsymbol{x}))^{\top}
    (\boldsymbol{y}-\hat{\boldsymbol{y}}(\boldsymbol{x}))
$$
其中$J$是标量损失值，$\boldsymbol{x}$是待优化参数，其梯度下降法更新公式为
$$
\boldsymbol{x}'=-\gamma\frac{\partial J}{\partial\boldsymbol{x}}
$$
为简化问题而可以令$\gamma=1$。$\hat{y}$与$x$的函数关系决定了矩阵的导数应该怎么求。按照导数定义先求$\Delta J$
$$
\begin{aligned}
    2\Delta J &= (\boldsymbol{y}-(\hat{\boldsymbol{y}}+\Delta\boldsymbol{y}))^{\top}
    (\boldsymbol{y}-(\hat{\boldsymbol{y}}+\Delta\boldsymbol{y}))
    -(\boldsymbol{y}-\hat{\boldsymbol{y}})^{\top}(\boldsymbol{y}-\hat{\boldsymbol{y}}) \\
    &= \Delta\boldsymbol{y}^{\top}\Delta\boldsymbol{y}
    -\Delta\boldsymbol{y}^{\top}(\boldsymbol{y}-\hat{\boldsymbol{y}})
    -(\boldsymbol{y}-\hat{\boldsymbol{y}})^{\top}\Delta\boldsymbol{y} \\
    &= -2(\boldsymbol{y}-\hat{\boldsymbol{y}})^{\top}\Delta\boldsymbol{y} \\
\end{aligned}
$$
令$\boldsymbol{e}=\boldsymbol{y}-\hat{\boldsymbol{y}}$，则
$$
\Delta J=-\boldsymbol{e}^{\top}\Delta\boldsymbol{y}
$$

1. $\boldsymbol{y}=\mathbf{A}\boldsymbol{x}$

$$
\begin{aligned}
    \Delta y_1=a_{11}\Delta x_1+a_{12}\Delta x_2 \\
    \Delta y_2=a_{21}\Delta x_1+a_{22}\Delta x_2 \\
\end{aligned}
$$
于是
$$
\begin{aligned}
    \Delta \boldsymbol{y} &= \mathbf{A}\Delta \boldsymbol{x} \\
    \Delta J &= -\boldsymbol{e}^{\top}\mathbf{A}\Delta \boldsymbol{x} \\
\end{aligned}
$$
把$\Delta J$拆开得到梯度下降法更新公式
$$
\begin{aligned}
    x_1' &= -\frac{\partial J}{\partial x_1}=e_1a_{11}+e_2a_{21} \\
    x_2' &= -\frac{\partial J}{\partial x_2}=e_1a_{12}+e_2a_{22} \\
\end{aligned}
$$
写成矩阵形式得
$$
\boldsymbol{x}'=\mathbf{A}^{\top}\boldsymbol{e}
$$
2. $\boldsymbol{y}=\boldsymbol{a}^{\top}\mathbf{X}\boldsymbol{b}$
$$
\left[\begin{matrix}
    y_1 \\ y_2
\end{matrix}\right]
=\left[\begin{matrix}
    a_1 & a_2 & a_3
\end{matrix}\right]
\left[\begin{matrix}
    x_{11} & x_{12} \\
    x_{21} & x_{22} \\
    x_{31} & x_{32} \\
\end{matrix}\right]
\left[\begin{matrix}
    b_1 \\ b_2
\end{matrix}\right]
$$

# 非线性系统的线性化
# 参考链接
有的直接是pdf文件，注意分辨。
[矩阵求导术(上)](https://zhuanlan.zhihu.com/p/24709748)
[matrix vector derivatives for machine learning](https://github.com/soloice/Matrix_Derivatives)
[Matrix Calculus](http://www.matrixcalculus.org/)
[Matrix Calculus: Derivation and Simple Application](https://project.hupili.net/tutorial/hu2012-matrix-calculus/hu2012matrix-calculus.pdf)
[Matrix Calculus - Notes on the Derivative of a Trace](http://paulklein.ca/newsite/teaching/matrix%20calculus.pdf)
[CS229 Lecture Notes](http://cs229.stanford.edu/notes2021fall/cs229-notes1.pdf)
[matrix calculus](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)
[关于Machine Learning中的基础数学推导值得一看的文章(持续更新)](https://zhuanlan.zhihu.com/p/475364267)
[Old and New Matrix Algebra Useful for Statistics](https://tminka.github.io/papers/matrix/)
