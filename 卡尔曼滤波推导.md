@[TOC]
本文从联合正态分布的一些性质出发推导标量和向量形式的卡尔曼滤波，以及重点推导了均值不为0的各个公式。下面的一些性质出于方便考虑给起了名字，不一定是正式的学术命名。
正式证明还没写完。

# 公式与符号说明
离散系统
$$
\begin{aligned}
& x(k)=Ax(k-1)+Bu(k-1)+v(k-1) \\
& y(k)=Cx(k)+W(k)
\end{aligned}
$$
其中$V,W$为零均值高斯白噪声的协方差矩阵。
卡尔曼滤波器递推公式如下
$$
\begin{aligned}
& \hat{x}(k|k-1)=A\hat{x}(k-1)+Bu(k-1) \\
& \hat{x}(k)=\hat{x}(k|k-1)+K(k)[y(k)-C\hat{x}(k|k-1)] \\
& P(k|k-1)=AP(k-1)A^{\text{T}}+V \\
& P(k)=[I-K(k)C]P(k|k-1) \\
& K(k)=P(k|k-1)C^{\text{T}}[CP(k|k-1)C^{\text{T}}+W]^{-1} \\
\end{aligned}
$$
- $\mathbf{Y}(k)=[y(0),y(1),\cdots,y(k)]$表示前$k$个时刻的观测数据
- $\hat{x}(k)=\text{E}[x(k)|Y(k)]$表示根据前$k$个时刻的观测数据对第$k$时刻的实际状态$x(k)$的预测值
- $\hat{x}(k|k-1)=\text{E}[x(k)|Y(k-1)]$表示根据前$k-1$个时刻的观测数据对第$k$时刻的实际状态$x(k)$的预测值
- $P(k|k-1)=\text{D}[x(k)-\hat{x}(k|k-1)]$称预测方差，或预测协方差矩阵
- $P(k)=\text{D}[x(k)-\hat{x}(k)]$称估计方差，或估计协方差矩阵
- $V=Dv(k)$为状态噪声方差或协方差矩阵
- $W=Dw(k)$为观测噪声方差或协方差矩阵

# 几个性质及部分证明
<font color=#2E8B57 size=5 face="KaiTi">贝叶斯最小均方误差估计量(Bmse)</font>
观测到$x$后$\theta$的最小均方误差估计量$\hat{\theta}$为
$$\hat{\theta}=\text{E}(\theta|x)$$
证明的主要思路是求$\hat{\theta}$使得最小均方误差最小。证明：
$$
\begin{aligned}
\text{Bmse}(\hat{\theta})
&= \text{E}[(\theta-\hat{\theta})^2] \\
&= \iint(\theta-\hat{\theta})^2p(x,\theta)\text{d}x\text{d}\theta \\
&=\int\left[\int(\theta-\hat{\theta})^2p(\theta|x)\text{d}\theta\right]
p(x)\text{d}x \\
J &= \int(\theta-\hat{\theta})^2p(\theta|x)\text{d}\theta \\
\frac{\partial J}{\partial\hat{\theta}}
&= -2\int\theta p(\theta|x)\text{d}\theta
+2\hat{\theta}\int p(\theta|x)\text{d}\theta=0 \\
\hat{\theta} &= \frac{2\displaystyle\int\theta p(\theta|x)\text{d}\theta}
{2\displaystyle\int p(\theta|x)\text{d}\theta}=\text{E}(\theta|x) \\
\end{aligned}
$$
<font color=#2E8B57 size=5 face="KaiTi">零均值应用定理(标量形式)</font>
设$x$和$y$为联合正态分布的随机变量，则
$$
\begin{aligned}
& \text{E}(y|x)=\text{E}y+\frac{\text{Cov}(x,y)}{\text{D}x}(x-\text{E}x) \\
& \text{D}(y|x)=\text{D}y-\frac{\text{Cov}^2(x,y)}{\text{D}x} \\
\end{aligned}
$$
两式可另外写作
$$
\begin{aligned}
& \frac{\hat{y}-\text{E}y}{\sqrt{\text{D}y}}
=\rho\frac{x-\text{E}x}{\sqrt{\text{D}x}} \\
& \text{D}(y|x)=\text{D}y(1-\rho^2) \\
\end{aligned}
$$
证明：
$$
\begin{aligned}
\hat{y} &= ax+b \\
J &= \text{E}(y-\hat{y})^2 \\
&= \text{E}[y^2-2y(ax+b)+(ax+b)^2] \\
&= y^2-2\text{E}xy\cdot a-2b\text{E}y
+\text{E}x^2\cdot a^2+2\text{E}x\cdot ab+b^2 \\
\text{d}J &= (-2\text{E}xy+2a\text{E}x^2+2b\text{E}x)\text{d}a
+(-2\text{E}y+2a\text{E}x+2b)\text{d}b \\
\frac{\partial J}{\partial b} &= -2\text{E}y+2a\text{E}x+2b = 0 \\
b &= \text{E}y-a\text{E}x \\
\frac{\partial J}{\partial a} &= -2\text{E}xy+2a\text{E}x^2+2b\text{E}x = 0 \\
a\text{E}x^2 &= \text{E}xy-b\text{E}x
= \text{E}xy-\text{E}x\text{E}y+a(\text{E}x)^2 \\
a &= \frac{\text{Cov}(x,y)}{\text{D}x} \\
\hat{y} &= ax+b = ax+\text{E}y-a\text{E}x = 
\text{E}y+\frac{\text{Cov}(x,y)}{\text{D}x}(x-\text{E}x) \\
\end{aligned}
$$
该定理只对包括正态分布在内的满足线性关系的随机变量有效(具体什么地方满足线性暂时没搞清楚)。例如，对两个联合均匀分布
$$
\begin{aligned}
& f(x,y)=2,\quad 0<x<1,0<y<x \\
& f(x,y)=3,\quad 0<x<1,x^2<y<\sqrt{x}
\end{aligned}
$$
第一个成立，第二个由于非线性的存在而不成立，也就是说$y$在数据$x$下的线性贝叶斯估计量不是最佳估计量，线性贝叶斯估计量和最佳贝叶斯估计量分别为
$$
\begin{aligned}
& \hat{y}=\text{E}y+\frac{\text{Cov}(x,y)}{\text{D}x}(x-\text{E}x)
=\frac{133x+9}{153} \\
& \hat{y}=\text{E}(y|x)=\frac{\sqrt{x}+x^2}{2} \\
\end{aligned}
$$
<font color=#2E8B57 size=5 face="KaiTi">零均值应用定理(向量形式)</font>
$\boldsymbol{x}$和$\boldsymbol{y}$为联合正态分布的随机向量，$\boldsymbol{x}$是m×1，$\boldsymbol{y}$是n×1，分块协方差矩阵
$$
\mathbf{C}=\left[\begin{matrix}
\mathbf{C}_{xx} & \mathbf{C}_{xy} \\ \mathbf{C}_{yx} & \mathbf{C}_{yy}
\end{matrix}\right]
$$
则
$$
\text{E}(\boldsymbol{y}|\boldsymbol{x})=\text{E}(\boldsymbol{y})
+\mathbf{C}_{yx}\mathbf{C}_{xx}^{-1}(\boldsymbol{x}-\text{E}(\boldsymbol{x}))
$$
其中$C_{xy}$表示$\text{Cov}(x,y)$。证明：
(其中省略的步骤见下文)
$$
\begin{aligned}
\hat{y} &= Ax+B \\
J &= \text{E}(y-\hat{y})^\top(y-\hat{y}) \\
&= \text{E}(y^\top y-2y^\top\hat{y}+\hat{y}^\top\hat{y}) \\
K &= y^\top y-2y^\top\hat{y}+\hat{y}^\top\hat{y} \\
\text{d}K &= \text{d}(-2y^\top(Ax+B)+(Ax+B)^\top(Ax+B)) \\
&= -2y^\top(\text{d}Ax+\text{d}B)
+\text{d}(x^\top A^\top Ax+2B^\top Ax+B^\top B) \\
&= -2xy^\top\text{d}A-2y^\top\text{d}B+2xx^\top A^\top\text{d}A \\
&+ 2x^\top A^\top\text{d}B+2xB^\top\text{d}A+2B^\top\text{d}B \\
&= (-2xy^\top+2xx^\top A^\top+2xB^\top)\text{d}A
+(-2y^\top+2x^\top A^\top+2B^\top)\text{d}B \\
\frac{\partial J}{\partial B} &= -2y+2Ax+2B =0\\
B &= \text{E}y-A\text{E}x \\
\frac{\partial J}{\partial A} &= -2yx^\top+2Axx^\top+2Bx^\top =0\\
\text{E}(Axx^\top) &= \text{E}(yx^\top-Bx^\top) \\
&= \text{E}(yx^\top-(\text{E}y-A\text{E}x)x^\top) \\
&= \text{E}yx^\top-\text{E}y\text{E}x^\top+A\text{E}x\text{E}x^\top \\
A &= (\text{E}yx^\top-\text{E}y\text{E}x^\top)
(\text{E}(xx^\top)-\text{E}x\text{E}x^\top)^{-1}
=C_{yx}C_{xx}^{-1} \\
\hat{y} &= C_{yx}C_{xx}^{-1}x+\text{E}y-C_{yx}C_{xx}^{-1}\text{E}x \\
&= \text{E}y+C_{yx}C_{xx}^{-1}(x-\text{E}x) \\
\end{aligned}
$$
其中矩阵求导的部分见 [矩阵求导术(上)](https://zhuanlan.zhihu.com/p/24709748)，其中的推导过程省略了迹的符号$\text{tr}$，注意分辨。另外因为3个符号$\text{d}$、$\text{E}$、$\text{tr}$均为线性算符，因此可交换计算顺序，推导中也省略了$\text{E}$。
上面推导中用到的一些矩阵微分与求迹公式详细推导如下：
$$
\begin{aligned}
\text{d}(B^\top B) &= \text{tr}(\text{d}B^\top B+B^\top\text{d}B) \\
&= \text{tr}(\text{d}B^\top B)+\text{tr}(B^\top\text{d}B) \\
&= \text{tr}(B^\top\text{d}B)+\text{tr}(B^\top\text{d}B) \\
&= \text{tr}(2B^\top\text{d}B) \\
\text{d}(x^\top A^\top Ax) &= \text{tr}(x^\top\text{d}(A^\top A)x) \\
&= \text{tr}(x^\top2A^\top\text{d}Ax) \\
&= \text{tr}(2xx^\top A^\top\text{d}A) \\
\text{d}(B^\top Ax) &= \text{tr}(\text{d}B^\top Ax+B^\top\text{d}Ax) \\
&= \text{tr}(x^\top A^\top\text{d}B+xB^\top\text{d}A) \\
\end{aligned}
$$

<font color=#2E8B57 size=5 face="KaiTi">投影定理(正交原理)</font>
![在这里插入图片描述](https://img-blog.csdnimg.cn/9691c2daf0c6492aa174112339530265.png#pic_center)
&emsp;&emsp;当利用数据样本的线性组合来估计一个随机变量的时候，当估计值与真实值的误差和每一个数据样本正交时，该估计值是最佳估计量，即数据样本$x$与最佳估计量$\hat{y}$满足
$$
\text{E}[(y-\hat{y})x^\top(n)]=0\quad n=0,1,\cdots,N-1
$$
&emsp;&emsp;零均值的随机变量满足内积空间中的性质。定义变量的长度$||x||=\sqrt{\text{E}x^2}$，变量$x$和$y$的内积$(x,y)$定义为$\text{E}(xy)$，两个变量的夹角定义为相关系数$\rho$。当$\text{E}(xy)=0$时称变量$x$和$y$正交。
&emsp;&emsp;均值不为零时，定义变量的长度$||x||=\sqrt{\text{D}x}$，变量$x$和$y$的内积$(x,y)$定义为$\text{Cov}(xy)$，两个变量的夹角定义为相关系数$\rho$。均值不为零的情况是我自己的猜测，很多资料都没有详细说明，但卡尔曼滤波的推导里全是均值非零的。
&emsp;&emsp;将$x$和$y$对应成数据的形式，即$x(0)$是$x$，$x(1)$是$y$，$\hat{x}(1|0)$是$\hat{y}$，得到
$$
\text{E}[(x(1)-\hat{x}(1|0))^\top x(0)]=0
$$
其中
$$
\widetilde{x}(k|k-1) = x(k)-\hat{x}(k|k-1)
$$
称为新息(innovation)，与旧数据$x(0)$正交。
&emsp;&emsp;标量形式证明：
$$
\begin{aligned}
\text{E}[x(\hat{y}-y)] &= \text{E}[x\text{E}y
+\frac{\text{Cov}(x,y)}{\text{D}x}(x^2-x\text{E}x)-xy] \\
&= \text{E}x\text{E}y
+\frac{\text{Cov}(x,y)}{\text{D}x}(\text{E}x^2-(\text{E}x)^2)-\text{E}xy \\
&= \text{Cov}(x,y)+\text{E}x\text{E}y-\text{E}xy=0
\end{aligned}
$$
&emsp;&emsp;向量形式证明：
$$
\begin{aligned}
\text{E}[(\hat{y}-y)x^\top]
&= \text{E}[\text{E}yx^\top+C_{yx}C_{xx}^{-1}(x-\text{E}x)x^\top-yx^\top] \\
&= \text{E}y\text{E}x^\top+\text{E}[C_{yx}C_{xx}^{-1}xx^\top]
-C_{yx}C_{xx}^{-1}\text{E}x\text{E}x^\top-\text{E}yx^\top \\
&= -C_{yx}+C_{yx}C_{xx}^{-1}(\text{E}xx^\top-\text{E}x\text{E}x^\top) \\
&= -C_{yx}+C_{yx}C_{xx}^{-1}C_{xx} \\
&= 0
\end{aligned}
$$
&emsp;&emsp;由于$\text{E}(y-\hat{y})=0$，所以均值非零时正交条件也恰好成立。由图可得投影定理的另一个公式
$$
\text{E}[(y-\hat{y})\hat{y}]=0
$$
证明：
$$
\begin{aligned}
& \text{E}[\hat{y}(\hat{y}-y)] \\
=& \text{E}[(\text{E} y+k x-k \text{E} x)(\text{E} y+k x-k \text{E} x-y)] \\
=& (\text{E}y)^{2}+k\text{E}x\text{E}y-k\text{E}x\text{E}y-(\text{E}y)^{2}\\
&+ k\text{E}x\text{E}y+k^{2}Ex^2-k^{2}(\text{E}x)^{2}-k\text{E}xy \\
&- k\text{E}x\text{E}y-k^{2}(\text{E}x)^{2}+k^{2}(\text{E}x)^{2}+k\text{E}x\text{E}y \\
=& k^{2}\text{D}x-k\text{Cov}(x,y) \\
=& \frac{[\text{Cov}(x,y)]^2}{[\text{D}x]^2}\text{D}x-\frac{\text{Cov}(x,y)}{\text{D}x}\text{Cov}(x,y) \\
=&0
\end{aligned}
$$
<font color=#2E8B57 size=5 face="KaiTi">期望可加性</font>
$\text{E}[y_1+y_2|x]=\text{E}[y_1|x]+\text{E}[y_2|x]$
<font color=#2E8B57 size=5 face="KaiTi">独立条件可加性</font>
若$x_1$和$x_2$独立，则
$$
\text{E}[y|x_1,x_2]=\text{E}[y|x_1]+\text{E}[y|x_2]-\text{E}y
$$
证明：
令$x=[x_1^\top,x_2^\top]^\top$，则
$$
\begin{aligned}
C_{xx}^{-1} &= \left[\begin{matrix}
C_{x_1x_1} & C_{x_1x_2} \\ C_{x_2x_1} & C_{x_2x_2}
\end{matrix}\right]^{-1}
= \left[\begin{matrix}
C_{x_1x_1}^{-1} & O \\ O & C_{x_2x_2}^{-1}
\end{matrix}\right] \\
C_{yx} &= \left[\begin{matrix}
C_{yx_1} & C_{yx_2}
\end{matrix}\right] \\
\text{E}(y|x) &= \text{E}y+C_{yx}C_{xx}^{-1}(x-\text{E}x) \\
&= \text{E}y+\left[\begin{matrix}
C_{yx_1} & C_{yx_2}
\end{matrix}\right]
\left[\begin{matrix}
C_{x_1x_1}^{-1} & O \\ O & C_{x_2x_2}^{-1}
\end{matrix}\right]
 \left[\begin{matrix}
x_1-\text{E}x_1 \\ x_2-\text{E}x_2
\end{matrix}\right] \\
&= \text{E}[y|x_1]+\text{E}[y|x_2]-\text{E}y
\end{aligned}
$$
<font color=#2E8B57 size=5 face="KaiTi">非独立条件可加性(新息定理)</font>
若$x_1$和$x_2$不独立，则根据投影定理取$x_2$与$x_1$独立的分量$\widetilde{x}_2$，满足
$$
\text{E}[y|x_1,x_2]
=\text{E}[y|x_1,\widetilde{x}_2]
=\text{E}[y|x_1]+\text{E}[y|\widetilde{x}_2]-\text{E}y
$$
其中$\widetilde{x}_2=x_2-\hat{x}_2=x_2-\text{E}(x_2|x_1)$，由投影定理，$x_1$与$\widetilde{x}_2$独立，$\widetilde{x}_2$称为**新息**。
证明：
(下面的每个式子是先求部分后求整体，为便于理解可以从下往上看)
(标量情况下，$C_{x_1x_2}=C_{x_2x_1}$)
$$
\begin{aligned}
\text{Cov}(y,\hat{x}_2) &= \text{E}[y\left(\text{E}x_2
+\frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}(x_1-\text{E}x_1)\right)]
-\text{E}y\text{E}\hat{x}_2 \\
&= \text{E}y\text{E}x_2+\frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}
(\text{E}x_1y-\text{E}x_1\text{E}y)-\text{E}y\text{E}x_2 \\
&= \frac{C_{x_1x_2}C_{yx_1}}{\text{D}x_1} \\
\text{Cov}(x_2,\hat{x}_2) &= \text{E}x_2\hat{x}_2-\text{E}x_2\text{E}\hat{x}_2 \\
&= \text{E}[x_2(\text{E}x_2+\frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}
(x_1-\text{E}x_1))]-(\text{E}x_2)^2 \\
&= \frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}(\text{E}x_2x_1-\text{E}x_2\text{E}x_1) \\
&= \frac{C_{x_1x_2}^2}{\text{D}x_1} \\
\text{D}\hat{x}_2 &= \text{D}(\text{E}x_2+\frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}
(x_1-\text{E}x_1)) \\
&= \frac{C_{x_1x_2}^2}{(\text{D}x_1)^2}\text{D}x_1
=\frac{C_{x_1x_2}^2}{\text{D}x_1} \\
\text{D}\widetilde{x}_2 &= \text{D}x_2+\text{D}\hat{x}_2
-2\text{Cov}(x_2,\hat{x}_2) \\
&= \text{D}x_2-\frac{C_{x_1x_2}^2}{\text{D}x_1} \\
\text{E}[y|x_1]+\text{E}[y|\widetilde{x}_2]-\text{E}y &= \text{E}y
+\frac{C_{x_1y}}{\text{D}x_1}(x_1-\text{E}x_1)
+\frac{\text{Cov}(y,\widetilde{x}_2)}{\text{D}\widetilde{x}_2}
(\widetilde{x}_2-\text{E}\widetilde{x}_2) \\
&= \text{OMIT}+\frac{\text{Cov}(y,x_2)-\text{Cov}(y,\hat{x}_2)}
{\text{D}\widetilde{x}_2}(x_2-\hat{x}_2) \\
&= \text{OMIT}+\frac{C_{yx_2}-\displaystyle\frac{C_{x_1x_2}C_{yx_1}}{\text{D}x_1}}
{\text{D}x_2-\displaystyle\frac{C_{x_1x_2}^2}{\text{D}x_1}}(x_2-\text{E}_2
-\frac{\text{Cov}(x_2,x_1)}{\text{D}x_1}(x_1-\text{E}_1)) \\
&= \text{OMIT}+\frac{C_{yx_2}\text{D}x_1-C_{x_1x_2}C_{yx_1}}
{\text{D}x_1\text{D}x_2-C_{x_1x_2}^2}(x_2-\text{E}_2
-\frac{C_{x_1x_2}}{\text{D}x_1}(x_1-\text{E}_1)) \\
&= \text{E}y+A(x_1-\text{E}x_1)
+B(x_2-\text{E}_2) \\
\end{aligned}
$$
其中$\text{OMIT}$用于代替式
$$
\text{E}y+\frac{C_{x_1y}}{\text{D}x_1}(x_1-\text{E}x_1)
$$
以及
$$
\begin{aligned}
B &= \frac{C_{yx_2}\text{D}x_1-C_{x_1x_2}C_{yx_1}}
{\text{D}x_1\text{D}x_2-C_{x_1x_2}^2} \\
A &= \frac{C_{x_1y}}{\text{D}x_1}-B\frac{C_{x_1x_2}}{\text{D}x_1} \\
&= \frac{\displaystyle\frac{C_{x_1y}}{\text{D}x_1}
(\text{D}x_1\text{D}x_2-C_{x_1x_2}^2)
-\displaystyle\frac{C_{x_1x_2}}{\text{D}x_1}
(C_{yx_2}\text{D}x_1-C_{x_1x_2}C_{yx_1})}
{\text{D}x_1\text{D}x_2-C_{x_1x_2}^2} \\
&= \frac{C_{x_1y}\text{D}x_2-C_{x_1x_2}C_{yx_2}}
{\text{D}x_1\text{D}x_2-C_{x_1x_2}^2} \\
\end{aligned}
$$
与另一个式子
$$
\begin{aligned}
\text{E}(y|x_1,x_2) &= \text{E}y+\left[
\begin{matrix}  C_{yx_1} & C_{yx_2}  \end{matrix}\right]
\left[\begin{matrix}
\text{D}x_1 & C_{x_1x_2} \\ C_{x_2x_1} & \text{D}x_2
\end{matrix}\right]^{-1}
\left[\begin{matrix}
x_1-\text{E}x_1 \\ x_2-\text{E}x_2
\end{matrix}\right] \\
&= \text{E}y+\frac{\left[
\begin{matrix}  C_{yx_1} & C_{yx_2}  \end{matrix}\right]}
{\text{D}x_1\text{D}x_2-C^2_{x_1x_2}}
\left[\begin{matrix}
\text{D}x_2 & -C_{x_1x_2} \\ -C_{x_1x_2} & \text{D}x_1
\end{matrix}\right]
\left[\begin{matrix}
x_1-\text{E}x_1 \\ x_2-\text{E}x_2
\end{matrix}\right] \\
\end{aligned}
$$
中对应的$A$和$B$相等。

# 高斯白噪声中的直流电平
&emsp;&emsp;这个例子可以作为铺垫，有助于理解卡尔曼滤波各个公式的来源，比如$x(k)$和$x(k-1)$之间为什么还要有一个$\hat{x}(k|k-1)$等。考虑模型
$$x(k)=A+w(k)$$
其中$A$是待估计参数，$w(k)$是均值为0、方差为$\sigma^2$的高斯白噪声，$x(k)$是观测。这里需要注意的是，$A$也是一个先验随机变量，也就是说在测量$A$之前，预先猜测比方说$A$应该在10左右，大概率不超过7\~13的范围，因此假设$A\sim N(10,1)$，然后开始测量。
&emsp;&emsp;一开始可以得到$\hat{x}(0)=x(0)$，然后根据$x(0)$和$x(1)$预测$k=1$时刻的值$\text{E}[x(1)|x(1),x(0)]$时，需要用到联合正态分布的条件可加性，但由于$x(1)$和$x(0)$不独立，需要使用投影定理计算出两个独立的变量$x(0)$与$\widetilde{x}(1|0)$，进而计算$\hat{x}(1)$，即
$$
\begin{aligned}
\hat{x}(1) &= \text{E}[x(1)|x(1),x(0)] \\
&= \text{E}[x(1)|x(0),\widetilde{x}(1|0)] \\
&= \text{E}[x(1)|x(0)]+\text{E}[x(1)|\widetilde{x}(1|0)]-\text{E}x(1)
\end{aligned}
$$
其中$\text{E}[x(1)|x(0)]=\hat{x}(1|0)$，
$$
\begin{aligned}
\hat{x}(1|0) &= \text{E}x(1)+\frac{\text{Cov}(x(1),x(0))}
{\text{D}x(0)}(x(0)-\text{E}x(0)) \\
&= A+\frac{\text{E}(A+w(1))(A+w(0))-\text{E}(A+w(1))\text{E}(A+w(0))}
{\text{E}(A+w(1))^2-[\text{E}(A+w(1))]^2}(x(0)-A) \\
&= A
\end{aligned}
$$
此时式中就出现了$\hat{x}(k|k-1)$，和另一个未知式$\text{E}[x(1)|\widetilde{x}(1|0)]$。由零均值应用定理，
$$
\text{E}[x(1)|\widetilde{x}(1|0)]
= \text{E}x(0)+\frac{\text{Cov}(x(1),\widetilde{x}(1|0))}
{\text{D}\widetilde{x}(1|0)}(\widetilde{x}(1|0)-\text{E}\widetilde{x}(1|0))
$$
其中
$$
\begin{aligned}
\widetilde{x}(1|0) &= x(1)-\hat{x}(1|0) \\
\text{E}\widetilde{x}(1|0) &= \text{E}(x-\hat{x}) = 0 \\
\text{D}\widetilde{x}(1|0)
&= \text{E}(x(1)-\hat{x}(1|0))^2 \\
&= \text{E}(A+w(1))^2 \\
&= a^2P(0)+\sigma^2 \\
&= P(1|0)
\end{aligned}
$$

# 卡尔曼滤波正式推导
<font color=#2E8B57 size=5 face="KaiTi">标量形式</font>
$$
\begin{aligned}
\hat{x}(k|k-1) &= \text{E}[x(k)|Y(k-1)] \\
&= \text{E}[Ax(k-1)+Bu(k-1)+v(k-1)|Y(k-1)] \\
&= A\text{E}[x(k-1)|Y(k-1)]+B\text{E}[u(k-1)|Y(k-1)]+\text{E}[v(k-1)|Y(k-1)] \\
&= A\hat{x}(k-1)+Bu(k-1) \\
\widetilde{x}(k|k-1) &= x(k)-\hat{x}(k|k-1) \\
&= [Ax(k-1)+Bu(k-1)+v(k-1)]-[A\hat{x}(k-1)+Bu(k-1)] \\
&= A\widetilde{x}(k-1)+v(k-1) \\
\hat{y}(k|k-1) &= \text{E}[y(k)|Y(k-1)] \\
&= \text{E}[Cx(k)+w(k-1)|Y(k-1)] \\
&= C\hat{x}(k|k-1) \\
\end{aligned}
$$
由条件可加性，
$$
\begin{aligned}
\hat{x}(k) &= \text{E}[x(k)|Y(k)] \\
 &= \text{E}[x(k)|Y(k-1),y(k)] \\
 &= \text{E}[x(k)|Y(k-1),\widetilde{y}(k|k-1)] \\
&= \text{E}[x(k)|Y(k-1)]+\text{E}[x(k)|\widetilde{y}(k|k-1)]-\text{E}x(k) \\
&= \hat{x}(k|k-1)+\text{E}[x(k)|\widetilde{y}(k|k-1)]-\text{E}x(k)
\end{aligned}
$$
由零均值应用定理，
$$
\text{E}[x(k)|\widetilde{y}(k|k-1)]
= \text{E}x(k)+\frac{\text{Cov}(x(k),\widetilde{y}(k|k-1))}
{\text{D}\widetilde{y}(k|k-1)}(\widetilde{y}(k|k-1)-\text{E}\widetilde{y}(k|k-1))
$$
其中
$$
\begin{aligned}
\text{E}\widetilde{y}(k|k-1) &= \text{E}(y-\hat{y}) = 0 \\
P(k|k-1) &= \text{D}\widetilde{x}(k|k-1) \\
&= \text{E}(a\widetilde{x}(k-1)+v(k-1))^2 \\
&= a^2P(k-1)+V
\end{aligned}
$$
令
$$
K(k)=\frac{\text{Cov}(x(k),\widetilde{y}(k|k-1))}
{\text{D}\widetilde{y}(k|k-1)}
$$
则
$$
\begin{aligned}
\text{E}[x(k)|\widetilde{y}(k|k-1)]
&= \text{E}x(k)+K(k)(y(k)-\hat{y}(k|k-1)) \\
\hat{x}(k)&= \hat{x}(k|k-1)+\text{E}[x(k)|\widetilde{y}(k|k-1)]-\text{E}x(k) \\
&= \hat{x}(k|k-1)+K(k)(y(k)-C\hat{x}(k|k-1))
\end{aligned}
$$
下面计算其中的$K(k)$，
$$
\begin{aligned}
\text{D}\widetilde{y}(k|k-1) &= \text{D}[Cx(k)+w(k)-C\hat{x}(k|k-1)] \\
&= C^2\text{D}[x(k)-\hat{x}(k|k-1)]+\text{D}w(k) \\
&= C^2P(k|k-1)+W \\
\text{Cov}(x(k),\widetilde{y}(k|k-1))
&= \text{Cov}(x(k),Cx(k)+w(k)-C\hat{x}(k|k-1)) \\
&= C\text{Cov}(x(k),x(k)-\hat{x}(k|k-1)) \\
&= C\text{E}[x(k)(x(k)-\hat{x}(k|k-1))]
-C\text{E}x(k)\text{E}[(x(k)-\hat{x}(k|k-1))] \\
&= C\text{E}[(x(k)((x(k)-\hat{x}(k|k-1))] \\
&= C\text{E}[(x(k)((x(k)-\hat{x}(k|k-1))
-\hat{x}(k|k-1)((x(k)-\hat{x}(k|k-1))] \\
&= C\text{E}[(x(k)-\hat{x}(k|k-1))^2] \\
&= CP(k|k-1) \\
K(k) &= \frac{\text{Cov}(x(k),\widetilde{y}(k|k-1))}
{\text{D}\widetilde{y}(k|k-1)} \\
&= \frac{CP(k|k-1)}{C^2P(k|k-1)+W} \\
\end{aligned}
$$
其中显然$C\text{E}x(k)\text{E}[(x(k)-\hat{x}(k|k-1))]=0$，而$\hat{x}(k|k-1)((x(k)-\hat{x}(k|k-1))]$是由投影定理的下面两个公式中的第二个
$$
\text{E}[y(x-\hat{x})]=0,\quad\text{E}[\hat{x}(x-\hat{x})]=0
$$
下面出于递推需要而计算$P(k)$
$$
\begin{aligned}
P(k) &= \text{D}[x(k)-\hat{x}(k)] \\
&= \text{D}[x(k)-\hat{x}(k|k-1)-K(k)(y(k)-C\hat{x}(k|k-1))] \\
&= \text{D}[x(k)-\hat{x}(k|k-1)-K(k)C(x(k)-\hat{x}(k|k-1))-K(k)w] \\
&= \text{D}[(1-K(k)C)\widetilde{x}-K(k)w] \\
&= (1-K(k)C)^2P(k|k-1)+K^2(k)W \\
&= \left(\frac{W}{C^2P(k|k-1)+W}\right)^2P(k|k-1)
+\left(\frac{CP(k|k-1)}{C^2P(k|k-1)+W}\right)^2W \\
&= \frac{W^2P(k|k-1)+C^2P^2(k|k-1)W}{(C^2P(k|k-1)+W)^2} \\
&= \frac{WP(k|k-1)}{C^2P(k|k-1)+W} \\
&= (1-K(k)C)P(k|k-1) \\
\end{aligned}
$$
<font color=#2E8B57 size=5 face="KaiTi">向量形式</font>
$$
\begin{aligned}
\hat{x}(k|k-1) &= \text{E}[x(k)|Y(k-1)] \\
&= \text{E}[Ax(k-1)+Bu(k-1)+v(k-1)|Y(k-1)] \\
&= A\text{E}[x(k-1)|Y(k-1)]+B\text{E}[u(k-1)|Y(k-1)]+\text{E}[v(k-1)|Y(k-1)] \\
&= A\hat{x}(k-1)+Bu(k-1) \\
\end{aligned}
$$
# 参考
- 孙增圻. 计算机控制理论与应用[M]. 清华大学出版社, 2008.
- StevenM.Kay, 罗鹏飞. 统计信号处理基础[M]. 电子工业出版社, 2014.
- 赵树杰, 赵建勋. 信号检测与估计理论[M]. 电子工业出版社, 2013.
- [卡尔曼滤波的推导过程详解](https://blog.csdn.net/hxlove123456/article/details/89056368)